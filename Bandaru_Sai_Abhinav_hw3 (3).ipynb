{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j59bbWA3L2z5"
   },
   "source": [
    "# CS 584 :: Data Mining :: George Mason University :: Fall 2022\n",
    "\n",
    "\n",
    "# Homework 3: Clustering&Association Rule Mining\n",
    "\n",
    "- **100 points [9% of your final grade]**\n",
    "- **Due Friday, November 4 by 11:59pm**\n",
    "\n",
    "- *Goals of this homework:* (1) implement your K-means model; and (2) implement the association rule mining process with the Apriori algorithm.\n",
    "\n",
    "- *Submission instructions:* for this homework, you only need to submit to Blackboard. Please name your submission **FirstName_Lastname_hw3.ipynb**, so for example, my submission would be something like **Ziwei_Zhu_hw3.ipynb**. Your notebook should be fully executed so that we can see all outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhOkQZKuL2z-"
   },
   "source": [
    "## Part 1: Clustering (50 points)\n",
    "\n",
    "In this part, you will implement your own K-means algorithm to conduct clustering on handwritten digit images. In this homework, we will still use the handwritten digit image dataset we have already used in previous homework. However, since clustering is unsupervised learning, which means we do not have the label information. So, here, we will only use the testing data stored in the \"test.txt\" file.\n",
    "\n",
    "First, let's load the data by excuting the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5gD5v-cTL2z_",
    "outputId": "a8eac0ce-23bb-4f85-ce6f-ff760d5ed1a3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of testing feature matrix: shape (10000, 784)\n",
      "array of testing label matrix: shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "\n",
    "test = np.loadtxt(\"test.txt\", delimiter=',')\n",
    "test_features = test[:, 1:]\n",
    "test_labels = test[:, 0]\n",
    "print('array of testing feature matrix: shape ' + str(np.shape(test_features)))\n",
    "print('array of testing label matrix: shape ' + str(np.shape(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTl84Fm4L20A"
   },
   "source": [
    "Now, it's time for you to implement your own K-means algorithm. First, please write your code to build your K-means model using the image data with **K = 10**, and **Euclidean distance**.\n",
    "\n",
    "**Note: You should implement the algorithm by yourself. You are NOT allowed to use Machine Learning libraries like Sklearn**\n",
    "\n",
    "**Note: you need to decide when to stop the model training process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f7phnkuL20B",
    "outputId": "cf2eae21-fc58-4801-ad0d-e88567a8d5a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of updated centroids\n",
      "(10, 784)\n",
      "total number of clusters are\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "class kmeansfunctions:\n",
    "    def __init__(self,data, k):\n",
    "        self.K = k # number of clusters\n",
    "        self.noofsamples, self.nooffeatures = data.shape #number of examples, number of features\n",
    "        \n",
    "    #function to initialize random centroids\n",
    "\n",
    "    \n",
    "    def index_of_minval(self,x):\n",
    "        return np.argmin(x)                  #this returns the indices of lowest value    \n",
    "    \n",
    "    \n",
    "    def cluster_prediction(self,main_clusters,data):\n",
    "        cluster_predictions = np.zeros(self.noofsamples) # taking rows and columns as zeros\n",
    "        for id_cluster, values_of_cluster in enumerate(main_clusters):\n",
    "            for values in values_of_cluster:\n",
    "                cluster_predictions[values] = id_cluster\n",
    "        return cluster_predictions    \n",
    "    \n",
    "    # function to create new clusters\n",
    "    def form_clusters(self,data, centroids):        # this function predicts clusters\n",
    "        clusters_list = [[] for _ in range(self.K)]\n",
    "        for pointid, point in enumerate(data):\n",
    "            closestcentroid = self.index_of_minval(np.sqrt(np.sum((point-centroids)**2, axis=1)))#using eulers distance for calculating centroid values\n",
    "            clusters_list[closestcentroid].append(pointid)\n",
    "        return clusters_list\n",
    "        \n",
    "    \n",
    "    def random_centroid_points(self, data):\n",
    "        centroid_finlist = np.zeros((self.K, self.nooffeatures))  \n",
    "        for k in range(self.K):\n",
    "            centroid_value = data[np.random.choice(range(self.noofsamples))] # random centroids\n",
    "            centroid_finlist[k] = centroid_value\n",
    "        return centroid_finlist \n",
    "    \n",
    "\n",
    "\n",
    "    # updating the initial centroids\n",
    "    def updating_centroids(self,main_cluster,data):\n",
    "        updated_centroids = np.zeros((self.K, self.nooffeatures)) # taking rows and columns as zeros\n",
    "        for idx, main_cluster in enumerate(main_cluster):\n",
    "            new_centroid_value = np.mean(data[main_cluster], axis=0) # find the value for new centroids\n",
    "            updated_centroids[idx] = new_centroid_value\n",
    "        return updated_centroids\n",
    "    \n",
    "                                            \n",
    "    \n",
    "        \n",
    "    # function to fit data\n",
    "    def fitting_data(self,data): \n",
    "        centroids = self.random_centroid_points(data)   #calling function to generate random centroids\n",
    "        for _ in range(100):\n",
    "            clusters = self.form_clusters(data, centroids) \n",
    "            prevcentroids = centroids\n",
    "            centroids = self.updating_centroids(clusters,data) \n",
    "            change = centroids - prevcentroids   #checking the difference values if there is any change or not\n",
    "            if not change.any():\n",
    "                break             #if new centroid -oldcentroids has no change then breaking the loop \n",
    "        predictions = self.cluster_prediction(clusters,data) \n",
    "        return predictions\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    k = 10                        #k indicates the number of clusters\n",
    "    data = test_features\n",
    "    kmeans_object = kmeansfunctions(data, k)\n",
    "    final_predictions = kmeans_object.fitting_data(data)\n",
    "    cent = kmeans_object.random_centroid_points(data)\n",
    "    cl = kmeans_object.form_clusters(data,cent)\n",
    "    newcent = kmeans_object.updating_centroids(cl,data)\n",
    "    print(\"shape of updated centroids\")\n",
    "    print(newcent.shape)     #printing shape of final centroids\n",
    "    #print(new_cent[:1,:])\n",
    "    #print (cl[:1])\n",
    "    print(\"total number of clusters are\")  #printing the total number of clusters\n",
    "    print (len(cl))\n",
    "\n",
    "  \n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RGBjsKUdL20C",
    "outputId": "ddfe90de-209e-4f0e-b656-944814bc877b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8. 1. ... 6. 7. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(final_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "stU_TOjRL20C"
   },
   "source": [
    "Next, you need to calculate the Sum of Squared Error (SSE) of each cluster generated by your K-means algorithm. Then, print out the averaged SSE of your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "taUSDunwL20D",
    "outputId": "d19fd9fa-282f-4fe0-9e7a-1d80f887b885"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2745061415.729097\n"
     ]
    }
   ],
   "source": [
    "# Sum of Squared Error calculation\n",
    "finalsse=0      #variable to store final sse\n",
    "final_clusters=cl\n",
    "final_centroids=newcent\n",
    "c=0\n",
    "for j in range(10):\n",
    "     for i in final_clusters[j]:\n",
    "        sse=np.sum((final_centroids[c]-test_features[i])**2)  #sse formula\n",
    "        finalsse=finalsse+sse\n",
    "   #print(c)\n",
    "     c=c+1\n",
    "print(finalsse/10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLnPw7OdL20D"
   },
   "source": [
    "Then, please have a look on https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_completeness_v_measure.html#sklearn.metrics.homogeneity_completeness_v_measure, and use this function to print out the homogeneity, completeness, and v-measure of your K-means model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxAQIHDhL20E",
    "outputId": "1c0087f4-be9a-434d-9996-92864c996c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homogenity: 0.5298832148456388\n",
      "completeness: 0.5451159984187467\n",
      "v-measure: 0.537391682043727\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "import sklearn\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure\n",
    "h,c,vm = sklearn.metrics.homogeneity_completeness_v_measure(test_labels, final_predictions)\n",
    "print ('homogenity:', h)\n",
    "print ('completeness:', c)\n",
    "print ('v-measure:', vm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oZlf19ML20E"
   },
   "source": [
    "Ok, now you already have a good model. But can you further improve it? In the next cell, please implement the K-means++ model introduced in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jrkaYEQaL20F",
    "outputId": "89f9839b-309b-40ce-8223-e07f746fadf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 784)\n",
      "10\n",
      "[7. 0. 4. ... 7. 9. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "class kmeans_plus:\n",
    "    def __init__(self,test, k):\n",
    "        self.K = k # number of clusters\n",
    "        self.noofsamples, self.nooffeatures = test.shape # number of examples and number of features\n",
    "        \n",
    "\n",
    "    \n",
    "    # creating clusters\n",
    "    def form_cluster(self,test, centroids):\n",
    "        clusters = [[] for _ in range(self.K)]\n",
    "        for pointid, point in enumerate(test):\n",
    "            closestcentroid = np.argmin(np.sqrt(np.sum((point-centroids)**2, axis=1)))\n",
    "            # closest centroid using euler distance equation(calculate distance of every point from centroid)\n",
    "            clusters[closestcentroid].append(pointid)\n",
    "        return clusters \n",
    "    \n",
    "    # new centroids\n",
    "    def new_centroids(self, cluster,test):\n",
    "        centroids = np.zeros((self.K, self.nooffeatures)) # row , column full with zero\n",
    "        for idx, cluster in enumerate(cluster):\n",
    "            newcentroid = np.mean(test[cluster], axis=0) # find the value for new centroids\n",
    "            centroids[idx] = newcentroid\n",
    "        return centroids\n",
    "    \n",
    "    def initial_random_centroids(self,test):   #this function generates random initial centriods fo kmeans++\n",
    "        centroids_list = []\n",
    "        centroids_list.append(test[np.random.randint(test.shape[0]),:])#taking random value points for centroid\n",
    "        for i in range(k-1):             \n",
    "            final_dist = []\n",
    "            for j in range(test.shape[0]):            \n",
    "                pt = test[j,:]\n",
    "                d = sys.maxsize\n",
    "                for z in range(len(centroids_list)):\n",
    "                    distance = np.sqrt(np.sum((pt-centroids_list[z])**2))  #calculating the distance from centroid to data points\n",
    "                    d = min(d,distance)\n",
    "                final_dist.append(d)\n",
    "            final_dist = np.array(final_dist)\n",
    "            nextcentroid = test[np.argmax(final_dist),:]\n",
    "            centroids_list.append(nextcentroid)\n",
    "            final_dist = []\n",
    "        return centroids_list    \n",
    "    \n",
    "    \n",
    "    # function to predict\n",
    "    def predict(self, clusters,test):\n",
    "        prediction_result = np.zeros(self.noofsamples) \n",
    "        for indx, cluster_value in enumerate(clusters):\n",
    "            for j in  cluster_value:\n",
    "                prediction_result[j] = indx\n",
    "        return prediction_result\n",
    "    \n",
    "        \n",
    "    # function to fit data\n",
    "    def fit_model(self,test):\n",
    "        # initialize random centroids\n",
    "        centroids = self.initial_random_centroids(test) \n",
    "        for _ in range(100):\n",
    "            clusters = self.form_cluster(test, centroids) \n",
    "            prev = centroids\n",
    "            updated_centroids = self.new_centroids(clusters,test) \n",
    "            change = updated_centroids - prev\n",
    "            if not change.any():\n",
    "                break\n",
    "        predictions = self.predict(clusters,test) \n",
    "        return predictions\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    #np.random.seed(10)\n",
    "    k = 10 # num of cluster\n",
    "    test = test_features\n",
    "    kmean_model = kmeans_plus(test, k)\n",
    "    final_prediction = kmean_model.fit_model(test)\n",
    "    centriods = kmean_model.initial_random_centroids(test)\n",
    "    clusters = kmean_model.form_cluster(test,centriods)\n",
    "    newcent = kmean_model.new_centroids(clusters,test)\n",
    "    print(newcent.shape)\n",
    "    #print(new_cent[:1,:])\n",
    "    #print (cl[:1])\n",
    "    print (len(clusters))\n",
    "    print(final_prediction)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxKDLbphL20F"
   },
   "source": [
    "In the next cell, use sklearn.metrics.homogeneity_completeness_v_measure() to print out the homogeneity, completeness, and v-measure of your K-means++ model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vw0YBTB8L20G",
    "outputId": "cee73508-b9b7-42c1-9d2f-4c878465886d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "homogenity: 0.23624960463842856\n",
      "completeness: 0.2747239469806676\n",
      "v-measure: 0.2540382908400449\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.metrics import homogeneity_completeness_v_measure\n",
    "h,c,vm = sklearn.metrics.homogeneity_completeness_v_measure(test_labels, final_prediction)\n",
    "print ('homogenity:', h)\n",
    "print ('completeness:', c)\n",
    "print ('v-measure:', vm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fawB2NzGL20G"
   },
   "source": [
    "### Question: Comparing the results by K-means and K-means++, do you see significant improvement? Write down in the next cell what do you think are the reasons for observing the improvement or not observing the improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A0OmISx3L20G"
   },
   "source": [
    "#### Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceNcRje7L20G"
   },
   "source": [
    "## Part 2: Association Rule Mining (50 points)\n",
    "\n",
    "In this part, you are going to examine movies using our understanding of association rules. For this part, you need to implement the apriori algorithm, and apply it to a movie rating dataset to find association rules of user-rate-movie behaviors. First, run the next cell to load the dataset we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "f2q3b5cnL20H",
    "outputId": "895868f9-b70a-44b3-f4df-b26ab902a5f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of user-movie matrix: shape (11743, 2)\n"
     ]
    }
   ],
   "source": [
    "user_movie_data = np.loadtxt(\"movie_rated.txt\", delimiter=',')\n",
    "print('array of user-movie matrix: shape ' + str(np.shape(user_movie_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KPVr6ULL20H"
   },
   "source": [
    "In this dataset, there are two columns: the first column is the integer ids of users, and the second column is the integer ids of movies. Each row denotes that the user of given user id rated the movie of the given movie id. We are going to treat each user as a transaction, so you will need to collect all the movies that have been rated by a single user as a transaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p3mACEKL20H"
   },
   "source": [
    "Now, you need to implement the apriori algorithm and apply it to this dataset to find association rules of user rating behaviors with **minimum support of 0.2** and **minimum confidence of 0.8**. We know there are many existing implementations of apriori online (check github for some good starting points). You are welcome to read existing codebases and let that inform your approach. \n",
    "\n",
    "**Note: Do not copy-paste any existing code.**\n",
    "\n",
    "**Note: We want your code to have sufficient comments to explain your steps, to show us that you really know what you are doing.**\n",
    "\n",
    "**Note: You should add print statements to print out the intermediate steps of your method -- e.g., the size of the candidate set at each step of the method, the size of the filtered set, and any other important information you think will highlight the method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "00Ej5x7VL20I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of user-movie matrix: shape (11743, 2)\n",
      "           0                                1\n",
      "0        1.0           Rosemary's Baby (1968)\n",
      "1        1.0  Children of a Lesser God (1986)\n",
      "2        1.0    Brothers McMullen, The (1995)\n",
      "3        1.0             Jurassic Park (1993)\n",
      "4        2.0           Rosemary's Baby (1968)\n",
      "...      ...                              ...\n",
      "11738  494.0   Star Trek: Insurrection (1998)\n",
      "11739  494.0            Wild Wild West (1999)\n",
      "11740  494.0          Schindler's List (1993)\n",
      "11741  494.0           Lethal Weapon 3 (1992)\n",
      "11742  494.0                      Dune (1984)\n",
      "\n",
      "[11743 rows x 2 columns]\n",
      "                                            FREQUENT 1-ITEMSET            \n",
      "[['Jurassic Park (1993)'], ['Godfather: Part II, The (1974)'], ['Back to the Future (1985)'], ['Groundhog Day (1993)'], ['Princess Bride, The (1987)'], ['Star Wars: Episode IV - A New Hope (1977)'], ['Rain Man (1988)'], [\"Schindler's List (1993)\"], ['Saving Private Ryan (1998)'], ['Who Framed Roger Rabbit? (1988)'], ['Godfather, The (1972)'], ['Contact (1997)'], ['Blair Witch Project, The (1999)'], ['Crying Game, The (1992)'], ['Chinatown (1974)'], ['Ghost (1990)'], ['Jaws (1975)'], ['League of Their Own, A (1992)'], ['American Pie (1999)'], ['Speed (1994)'], ['Abyss, The (1989)']]\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "user_movie_data = np.loadtxt(\"movie_rated.txt\", delimiter=',')  #loading and saving data from movie_rated.txt to user_movie_data\n",
    "daf=pd.read_csv('movies.csv')            #reading movie ids and names from movies.csv\n",
    "print('array of user-movie matrix: shape ' + str(np.shape(user_movie_data)))\n",
    "user=user_movie_data[:,:1]     \n",
    "movies=user_movie_data[:,1:]\n",
    "updated_movie_list=pd.DataFrame(user_movie_data)   #converting user_movie_data to data frame\n",
    "modifiedmovies=[]    #list to store movie names and mapping them in order according to movie ids in movie_rated.txt\n",
    "for i in movies:\n",
    "    temp=int(i)\n",
    "    for ind in daf.index:\n",
    "        temp1=daf['movieId'][ind]\n",
    "        if(temp1==temp):   #condition to check if movie id matches if it matches replacing movie id with movie name\n",
    "            add=daf['movie_name'][ind]\n",
    "            modifiedmovies.append(add)  #adding movie name to list which matches with movie id in movie_rated.txt\n",
    "#print(modifiedmovies) \n",
    "updated_movie_list[1]=modifiedmovies  #updating the usermoviedata frame\n",
    "\n",
    "#print(user_movie_data)\n",
    "#print(\"g\")\n",
    "import itertools\n",
    "print(updated_movie_list)\n",
    "final_movie_list=updated_movie_list.values.tolist()  #converting dataframe to list\n",
    "df={}\n",
    "for key,val in final_movie_list:\n",
    "     df.setdefault(key, []).append(val)      #saving values to dictionary\n",
    "\n",
    "mainlist=[]\n",
    "j=0\n",
    "for i in range(1,495):\n",
    "    mainlist.append(df[i])            #appending values from dictionary to mainlist variable\n",
    "    j+=1\n",
    " \n",
    "\n",
    " #print(mainlist)    \n",
    "confidence = 0.8\n",
    "support = 0.2                         #taking support and confidence values as 0.2,0.8\n",
    "\n",
    "dict_list = {}\n",
    "total_transact = 0\n",
    "dlist = []\n",
    "tlist = []\n",
    "datalist=mainlist\n",
    "for val_list in datalist:\n",
    "    tlist = []                    #iterating through datalist which contains users and user rated movie names\n",
    "    total_transact += 1\n",
    "    for val in val_list:       #iterating through individual movie names\n",
    "        tlist.append(val) \n",
    "        if val not in dict_list.keys(): \n",
    "            dict_list[val] = 1\n",
    "        else:\n",
    "            c_value = dict_list[val]\n",
    "            dict_list[val] = c_value + 1\n",
    "    dlist.append(tlist)\n",
    "\n",
    "List_one = []\n",
    "for key in dict_list:\n",
    "    if (100 * dict_list[key]/total_transact) >= support*100:  #checking if support is greater than 0.2\n",
    "        list = []\n",
    "        list.append(key)\n",
    "        List_one.append(list)   #appending frequent itemset 1 to List_one\n",
    "print (\"                                            FREQUENT 1-ITEMSET            \")\n",
    "print (List_one)\n",
    "print (\"  \")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def not_freq_subset(temp,dat_list_one, k):  #function returns true if its not a frequent subset\n",
    "    item_list = []\n",
    "    item_list = search_subset(temp,k)   #this is used to iterate through subset\n",
    "    for item in item_list: \n",
    "        sort_list = []              \n",
    "        for l in item:\n",
    "            sort_list.append(l)\n",
    "        sort_list.sort()\n",
    "        if sort_list not in dat_list_one:    #return true if not a frequent subset\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def search_subset(ab,cd):\n",
    "    return set(itertools.combinations(ab,cd))     #searching for subset\n",
    " \n",
    "def main_apriori(dat_list_one, k):\n",
    "    k_len = k                          \n",
    "    aprior_list = []    \n",
    "    for list1 in dat_list_one:  #iterating through dat_list_one\n",
    "        for list2 in dat_list_one:                 \n",
    "            c_value = 0\n",
    "            temp = []\n",
    "            if list1 != list2:      #checking if list1 is not equal to list2\n",
    "                while c_value < k_len-1:\n",
    "                    if list1[c_value] != list2[c_value]:\n",
    "                        break\n",
    "                    else:\n",
    "                        c_value += 1\n",
    "                else:\n",
    "                    if list1[k_len-1] < list2[k_len-1]:\n",
    "                        for item in list1:\n",
    "                            temp.append(item)\n",
    "                        temp.append(list2[k_len-1])\n",
    "                        if not not_freq_subset(temp,dat_list_one, k):    #checking if data is not a frequent sub set\n",
    "                            aprior_list.append(temp) \n",
    "                            temp = []\n",
    "    return aprior_list\n",
    "\n",
    "\n",
    "\n",
    "def gen_freq_subsets():\n",
    "    k = 2\n",
    "    dat_list_one = []\n",
    "    fq_item = []          #this function is used to generate frequent item set\n",
    "    frequent_item_list = []\n",
    "    c_value = 0\n",
    "    total_transact = 0\n",
    "    for item in List_one:\n",
    "        dat_list_one.append(item)\n",
    "    while dat_list_one != []:\n",
    "        aprior_list = []\n",
    "        fq_item = []\n",
    "        aprior_list = main_apriori(dat_list_one, k-1)  #calling main apriori function\n",
    "        \n",
    "        for i in aprior_list:         #iterating through aprior list\n",
    "            c_value = 0\n",
    "            total_transact = 0\n",
    "            j = set(i)\n",
    "            for tlist in dlist:\n",
    "                total_transact += 1\n",
    "                t_val_item = set(tlist)\n",
    "                if j.issubset(t_val_item) == True:\n",
    "                    c_value += 1\n",
    "            if (100 * c_value/total_transact) >= support*100:  #checking if support is greater than 0.2\n",
    "                i.sort()\n",
    "                fq_item.append(i)       #if support is greater append to frequent item list\n",
    "        dat_list_one = []\n",
    "        print (\"                                         FREQUENT %d-ITEMSET             \" % k)\n",
    "        print (fq_item)             #printing frequent item set list\n",
    "        print (\"                                          \")\n",
    "        for l in fq_item:\n",
    "            dat_list_one.append(l)\n",
    "        k += 1\n",
    "        if fq_item != []:\n",
    "            frequent_item_list.append(fq_item)      #if frequent item set is not in list appending the itemset to final frequent item list\n",
    "    \n",
    "    return frequent_item_list\n",
    "     \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrf00e3zL20I"
   },
   "source": [
    "Finally, print your final association rules in the following format:\n",
    "\n",
    "**movie_name_1, movie_name_2, ... --> movie_name_k**\n",
    "\n",
    "where the movie names can be fetched by joining the movieId with the file 'movies.csv'. For example, one rule that you should find is:\n",
    "\n",
    "**Jurassic Park (1993), Back to the Future (1985) --> Star Wars: Episode IV - A New Hope (1977)**\n",
    "\n",
    "**Hint: You may need to use the Pandas library to load and process the movies.csv file, such as use pandas.read_csv() to load the data. https://pandas.pydata.org/pandas-docs/dev/user_guide/10min.html is a good place to learn the basics about Pandas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hR1aTvOPL20I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         FREQUENT 2-ITEMSET             \n",
      "[['Jurassic Park (1993)', 'Princess Bride, The (1987)'], ['Jurassic Park (1993)', 'Star Wars: Episode IV - A New Hope (1977)'], ['Jurassic Park (1993)', \"Schindler's List (1993)\"], ['Jurassic Park (1993)', 'Saving Private Ryan (1998)'], ['Jurassic Park (1993)', 'Speed (1994)'], ['Godfather: Part II, The (1974)', 'Star Wars: Episode IV - A New Hope (1977)'], ['Back to the Future (1985)', 'Jurassic Park (1993)'], ['Back to the Future (1985)', 'Groundhog Day (1993)'], ['Back to the Future (1985)', 'Princess Bride, The (1987)'], ['Back to the Future (1985)', 'Star Wars: Episode IV - A New Hope (1977)'], ['Back to the Future (1985)', \"Schindler's List (1993)\"], ['Back to the Future (1985)', 'Saving Private Ryan (1998)'], ['Back to the Future (1985)', 'Who Framed Roger Rabbit? (1988)'], ['Back to the Future (1985)', 'Godfather, The (1972)'], ['Groundhog Day (1993)', 'Jurassic Park (1993)'], ['Groundhog Day (1993)', 'Princess Bride, The (1987)'], ['Groundhog Day (1993)', 'Star Wars: Episode IV - A New Hope (1977)'], ['Groundhog Day (1993)', \"Schindler's List (1993)\"], ['Groundhog Day (1993)', 'Saving Private Ryan (1998)'], ['Princess Bride, The (1987)', 'Star Wars: Episode IV - A New Hope (1977)'], ['Princess Bride, The (1987)', \"Schindler's List (1993)\"], ['Princess Bride, The (1987)', 'Saving Private Ryan (1998)'], ['Princess Bride, The (1987)', 'Who Framed Roger Rabbit? (1988)'], ['Star Wars: Episode IV - A New Hope (1977)', 'Who Framed Roger Rabbit? (1988)'], [\"Schindler's List (1993)\", 'Star Wars: Episode IV - A New Hope (1977)'], ['Saving Private Ryan (1998)', 'Star Wars: Episode IV - A New Hope (1977)'], ['Saving Private Ryan (1998)', \"Schindler's List (1993)\"], ['Godfather, The (1972)', 'Jurassic Park (1993)'], ['Godfather, The (1972)', 'Godfather: Part II, The (1974)'], ['Godfather, The (1972)', 'Princess Bride, The (1987)'], ['Godfather, The (1972)', 'Star Wars: Episode IV - A New Hope (1977)'], ['Godfather, The (1972)', \"Schindler's List (1993)\"], ['Godfather, The (1972)', 'Saving Private Ryan (1998)'], ['Jaws (1975)', 'Star Wars: Episode IV - A New Hope (1977)'], ['Speed (1994)', 'Star Wars: Episode IV - A New Hope (1977)'], ['Abyss, The (1989)', 'Star Wars: Episode IV - A New Hope (1977)']]\n",
      "                                          \n",
      "                                         FREQUENT 3-ITEMSET             \n",
      "[['Jurassic Park (1993)', 'Princess Bride, The (1987)', 'Star Wars: Episode IV - A New Hope (1977)'], ['Jurassic Park (1993)', 'Saving Private Ryan (1998)', 'Star Wars: Episode IV - A New Hope (1977)'], ['Back to the Future (1985)', 'Jurassic Park (1993)', 'Star Wars: Episode IV - A New Hope (1977)'], ['Back to the Future (1985)', 'Groundhog Day (1993)', 'Princess Bride, The (1987)'], ['Back to the Future (1985)', 'Groundhog Day (1993)', 'Star Wars: Episode IV - A New Hope (1977)'], ['Back to the Future (1985)', 'Princess Bride, The (1987)', 'Star Wars: Episode IV - A New Hope (1977)'], ['Back to the Future (1985)', \"Schindler's List (1993)\", 'Star Wars: Episode IV - A New Hope (1977)'], ['Back to the Future (1985)', 'Saving Private Ryan (1998)', 'Star Wars: Episode IV - A New Hope (1977)'], ['Groundhog Day (1993)', 'Princess Bride, The (1987)', 'Star Wars: Episode IV - A New Hope (1977)'], ['Princess Bride, The (1987)', 'Saving Private Ryan (1998)', 'Star Wars: Episode IV - A New Hope (1977)'], ['Saving Private Ryan (1998)', \"Schindler's List (1993)\", 'Star Wars: Episode IV - A New Hope (1977)'], ['Godfather, The (1972)', 'Godfather: Part II, The (1974)', 'Star Wars: Episode IV - A New Hope (1977)']]\n",
      "                                          \n",
      "                                         FREQUENT 4-ITEMSET             \n",
      "[]\n",
      "                                          \n",
      "                              Association rules                    \n",
      " \n",
      " 1 Rule : ['Godfather: Part II, The (1974)'] --> ['Godfather, The (1972)']\n",
      " 2 Rule : ['Jaws (1975)'] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      " 3 Rule : ['Jurassic Park (1993)', 'Princess Bride, The (1987)'] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      " 4 Rule : ['Jurassic Park (1993)', 'Saving Private Ryan (1998)'] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      " 5 Rule : ['Back to the Future (1985)', 'Jurassic Park (1993)'] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      " 6 Rule : ['Groundhog Day (1993)', 'Princess Bride, The (1987)'] --> ['Back to the Future (1985)']\n",
      " 7 Rule : ['Groundhog Day (1993)', 'Star Wars: Episode IV - A New Hope (1977)'] --> ['Back to the Future (1985)']\n",
      " 8 Rule : ['Back to the Future (1985)', 'Princess Bride, The (1987)'] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      " 9 Rule : ['Princess Bride, The (1987)', 'Star Wars: Episode IV - A New Hope (1977)'] --> ['Back to the Future (1985)']\n",
      " 10 Rule : ['Back to the Future (1985)', \"Schindler's List (1993)\"] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      " 11 Rule : ['Back to the Future (1985)', 'Saving Private Ryan (1998)'] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      " 12 Rule : ['Princess Bride, The (1987)', 'Saving Private Ryan (1998)'] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      " 13 Rule : ['Godfather, The (1972)', 'Godfather: Part II, The (1974)'] --> ['Star Wars: Episode IV - A New Hope (1977)']\n",
      " 14 Rule : ['Godfather: Part II, The (1974)', 'Star Wars: Episode IV - A New Hope (1977)'] --> ['Godfather, The (1972)']\n"
     ]
    }
   ],
   "source": [
    "# Write your code to print out the rules\n",
    "\n",
    "def associationrules():    #this function is used to generate associationrules\n",
    "    item_rel = []\n",
    "    subsets_list = []\n",
    "    k_len = 0\n",
    "    c_value = 1\n",
    "    increment_one = 0\n",
    "    increment_val_two = 0\n",
    "    rule_number = 1\n",
    "    subset_rel = []\n",
    "    freq_itemset_val= gen_freq_subsets()   #calling function and storing frequent itemsets\n",
    "    print (\"                              Association rules                    \")\n",
    "    print(\" \")\n",
    "    for freq_val_record in freq_itemset_val:     #iterating through frequent item record\n",
    "        for freq_val in freq_val_record:      #iterating through data in frequent item record\n",
    "            k_len = len(freq_val)\n",
    "            c_value = 1\n",
    "            while c_value < k_len: \n",
    "                item_rel = []\n",
    "                subsets_list = search_subset(freq_val,c_value)      \n",
    "                c_value += 1\n",
    "                for item in subsets_list:       #iterating through subsets_list\n",
    "                    increment_one = 0        \n",
    "                    increment_val_two = 0             \n",
    "                    item_rel = []\n",
    "                    subset_rel = []\n",
    "                    for i in item:    #iterating through item\n",
    "                        item_rel.append(i)\n",
    "                    for tlist in dlist:   \n",
    "                        if set(item_rel).issubset(set(tlist)) == True:\n",
    "                            increment_one += 1\n",
    "                        if set(freq_val).issubset(set(tlist)) == True:\n",
    "                            increment_val_two += 1\n",
    "                    if 100*increment_val_two/increment_one >= confidence*100: #checking if confidence is greater than 0.8\n",
    "                        for match_freq in freq_val:\n",
    "                            if match_freq not in item_rel:\n",
    "                                subset_rel.append(match_freq)  #appending the itemset with relation to subset_rel\n",
    "                        print (\" %d Rule : %s --> %s\" %(rule_number,item_rel,subset_rel))\n",
    "                        rule_number =rule_number+1  \n",
    "\n",
    "associationrules()   \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
